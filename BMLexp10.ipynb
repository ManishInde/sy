{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI6RH7yybpf4aqyuTZET9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManishInde/sy/blob/main/BMLexp10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrJL29nMtbc6",
        "outputId": "cce1fe42-855a-4f57-e129-f82f703cd873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   preg  plas  pres  skin  insu  mass   pedi  age            class\n",
            "0     6   148    72    35     0  33.6  0.627   50  tested_positive\n",
            "1     1    85    66    29     0  26.6  0.351   31  tested_negative\n",
            "2     8   183    64     0     0  23.3  0.672   32  tested_positive\n",
            "3     1    89    66    23    94  28.1  0.167   21  tested_negative\n",
            "4     0   137    40    35   168  43.1  2.288   33  tested_positive\n",
            "Accuracy: 0.7316\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "tested_negative       0.81      0.77      0.79       151\n",
            "tested_positive       0.60      0.66      0.63        80\n",
            "\n",
            "       accuracy                           0.73       231\n",
            "      macro avg       0.71      0.72      0.71       231\n",
            "   weighted avg       0.74      0.73      0.73       231\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch the dataset directly from UCI Machine Learning Repository (using OpenML)\n",
        "diabetes_data = fetch_openml(name=\"diabetes\", version=1)\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "df = diabetes_data.frame\n",
        "\n",
        "# Check first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Separate the features (X) and the target (y)\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a DecisionTreeClassifier as the base model\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize the BaggingClassifier with the DecisionTreeClassifier as the base estimator\n",
        "# Use 'estimator' instead of 'base_estimator' for newer versions of scikit-learn\n",
        "bagging = BaggingClassifier(estimator=dtree, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n",
        "\n",
        "# Fetch the dataset directly from UCI Machine Learning Repository (using OpenML)\n",
        "diabetes_data = fetch_openml(name=\"diabetes\", version=1)\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "df = diabetes_data.frame\n",
        "\n",
        "# Separate the features (X) and the target (y)\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features to ensure better convergence for Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the base classifiers\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),  # Increased max_iter\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "# Loop through each classifier, apply Bagging, and evaluate performance\n",
        "for model_name, model in classifiers.items():\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "\n",
        "    # Apply Bagging with each base classifier\n",
        "    bagging = BaggingClassifier(estimator=model, n_estimators=100, random_state=42)  # Changed 'base_estimator' to 'estimator'\n",
        "\n",
        "    # Train the model\n",
        "    bagging.fit(X_train_scaled, y_train)  # Use scaled data for training\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = bagging.predict(X_test_scaled)  # Use scaled data for prediction\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Print the classification report (only precision, recall, f1-score)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # Dynamically extract the class labels from the classification report\n",
        "    for class_label in report.keys():\n",
        "        if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "            precision = report[class_label]['precision']\n",
        "            recall = report[class_label]['recall']\n",
        "            f1 = report[class_label]['f1-score']\n",
        "            print(f\"Class {class_label} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13n7kfoixQoa",
        "outputId": "0c152d19-bddf-4eba-b08e-1721261958cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model: Decision Tree\n",
            "Accuracy: 0.7359\n",
            "Class tested_negative - Precision: 0.8125, Recall: 0.7748, F1-Score: 0.7932\n",
            "Class tested_positive - Precision: 0.6092, Recall: 0.6625, F1-Score: 0.6347\n",
            "\n",
            "Evaluating model: K-Nearest Neighbors\n",
            "Accuracy: 0.6840\n",
            "Class tested_negative - Precision: 0.7468, Recall: 0.7815, F1-Score: 0.7638\n",
            "Class tested_positive - Precision: 0.5479, Recall: 0.5000, F1-Score: 0.5229\n",
            "\n",
            "Evaluating model: Support Vector Machine\n",
            "Accuracy: 0.7446\n",
            "Class tested_negative - Precision: 0.7911, Recall: 0.8278, F1-Score: 0.8091\n",
            "Class tested_positive - Precision: 0.6438, Recall: 0.5875, F1-Score: 0.6144\n",
            "\n",
            "Evaluating model: Logistic Regression\n",
            "Accuracy: 0.7229\n",
            "Class tested_negative - Precision: 0.7881, Recall: 0.7881, F1-Score: 0.7881\n",
            "Class tested_positive - Precision: 0.6000, Recall: 0.6000, F1-Score: 0.6000\n",
            "\n",
            "Evaluating model: Naive Bayes\n",
            "Accuracy: 0.7446\n",
            "Class tested_negative - Precision: 0.8151, Recall: 0.7881, F1-Score: 0.8013\n",
            "Class tested_positive - Precision: 0.6235, Recall: 0.6625, F1-Score: 0.6424\n"
          ]
        }
      ]
    }
  ]
}